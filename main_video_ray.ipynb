{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cv2 import dnn_superres\n",
    "import moviepy\n",
    "import moviepy.editor\n",
    "import os\n",
    "\n",
    "file_name = 'low_res_2' # 'Megan Is Missing'\n",
    "ext = 'mp4'\n",
    "scale = 4\n",
    "input_video = f'./input/videos/{file_name}.{ext}'\n",
    "output_root_path = f'./output/videos/{file_name}_multiproc'\n",
    "\n",
    "os.makedirs(output_root_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:07:22,338\tINFO worker.py:1625 -- Started a local Ray instance.\n",
      "2023-06-03 12:07:22,362\tINFO packaging.py:520 -- Creating a file package for local directory './'.\n",
      "2023-06-03 12:07:22,384\tINFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_b6bdab388c25f83b.zip' (0.09MiB) to Ray cluster...\n",
      "2023-06-03 12:07:22,387\tINFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_b6bdab388c25f83b.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 64, 23, 32, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import ray\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('./input/BSRGAN'))\n",
    "from models.network_rrdbnet import RRDBNet as net\n",
    "\n",
    "threads = 3\n",
    "runtime_env = {\"working_dir\": \"./\", \"conda\": \"env_pytorch\"}\n",
    "num_cpus = threads\n",
    "num_gpus = 1/threads\n",
    "ray.init(runtime_env=runtime_env, num_cpus=4, num_gpus=1)\n",
    "\n",
    "model_name = 'BSRGAN'\n",
    "modelScale = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = f'./input/models/{model_name}.pth'\n",
    "time_limit = 1*60 # seconds\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = net(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, sf=modelScale)  # define network\n",
    "model.load_state_dict(torch.load(model_path), strict=True)\n",
    "model = model.to(device)\n",
    "modelObjRef = ray.put(model)\n",
    "modelObjRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_path(file_name, model_name, file_part, parent_path, ext):\n",
    "    file_prefix = f'{file_name}_{model_name}_{file_part}'\n",
    "    output_path = f'{parent_path}/{file_prefix}.{ext}'\n",
    "    return output_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split input video into #threads number of parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcap = cv2.VideoCapture(input_video)\n",
    "input_video_parts_root = f'./input/videos/{file_name}_parts'\n",
    "os.makedirs(input_video_parts_root, exist_ok=True)\n",
    "\n",
    "total_frames = vcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "frames_per_part = total_frames//threads\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "frame_rate = vcap.get(cv2.CAP_PROP_FPS)\n",
    "success, frame = vcap.read()\n",
    "frame_count = 0\n",
    "vcap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "input_width = frame.shape[1]\n",
    "input_height = frame.shape[0]\n",
    "out_resolution = (input_width, input_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to ./input/videos/low_res_2_parts/low_res_2_part0.mp4\n",
      "writing to ./input/videos/low_res_2_parts/low_res_2_part1.mp4\n",
      "writing to ./input/videos/low_res_2_parts/low_res_2_part2.mp4\n",
      "total frames in input=913.0, total frames in parts=913.0\n"
     ]
    }
   ],
   "source": [
    "for part in range(threads):\n",
    "    part_path = f'{input_video_parts_root}/{file_name}_part{part}.{ext}'\n",
    "    print(f'writing to {part_path}')\n",
    "    vwriter = cv2.VideoWriter(part_path, codec,\n",
    "                         frame_rate, out_resolution)\n",
    "    start = int(frames_per_part * part)\n",
    "    end = int(start + frames_per_part if part < threads-1 else total_frames)\n",
    "    for i in range(start, end):\n",
    "        success, frame = vcap.read()\n",
    "        vwriter.write(frame)\n",
    "    \n",
    "    vwriter.release()\n",
    "\n",
    "vcap.release()\n",
    "\n",
    "file_parts = os.listdir(input_video_parts_root)\n",
    "num_frames = 0\n",
    "for file_part_ in file_parts:\n",
    "    vs = cv2.VideoCapture(f'{input_video_parts_root}/{file_part_}')\n",
    "    num_frames += vs.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "print(f'total frames in input={total_frames}, total frames in parts={num_frames}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@ray.remote(num_gpus=num_gpus, num_cpus=num_cpus)\n",
    "class WorkerActor(object):\n",
    "    def __init__(self, model, video_path, output_path) -> None:\n",
    "        self.model = model #.to('cuda')\n",
    "        self.input_path = video_path\n",
    "        self.output_path = output_path\n",
    "\n",
    "    def upscale(self):\n",
    "        codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        output_part_path = self.output_path  # get_output_file_path(file_name, model_name, f'part{part}', output_root_path, ext)\n",
    "        \n",
    "        input_part_path = self.input_path  # f'{input_video_parts_root}/{file_name}_part{part}.{ext}'\n",
    "        vs = cv2.VideoCapture(input_part_path)\n",
    "        total_frames = vs.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        print(vs, input_part_path, total_frames)\n",
    "        # vs.set(cv2.CAP_PROP_POS_FRAMES, number_of_frames_completed)\n",
    "        success, frame = vs.read()\n",
    "        input_width = frame.shape[1] # 640\n",
    "        input_height = frame.shape[0] # 352\n",
    "        frame_count = 0 # number_of_frames_completed\n",
    "        frame_rate = vs.get(cv2.CAP_PROP_FPS)\n",
    "        out_resolution = (modelScale*input_width, modelScale*input_height)\n",
    "\n",
    "        out = cv2.VideoWriter(output_part_path, codec,\n",
    "                            frame_rate, out_resolution)\n",
    "\n",
    "        print(f'input res: {frame.shape}, total frames={total_frames},\\\n",
    "            frame rate={frame_rate}, scale={modelScale},\\\n",
    "            outpath={output_part_path}')\n",
    "\n",
    "        # with prof(period=0.001):\n",
    "        with tqdm(total=total_frames) as pbar:\n",
    "            # pbar.update(number_of_frames_completed)\n",
    "\n",
    "            while success:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = util.uint2tensor4(frame).to(device)\n",
    "                upsampled_frame = model(frame)\n",
    "                upsampled_frame = util.tensor2uint(upsampled_frame)\n",
    "                upsampled_frame = cv2.cvtColor(upsampled_frame, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                out.write(upsampled_frame)\n",
    "                frame_count += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f'frame no: {frame_count}')\n",
    "                success, frame = vs.read()\n",
    "                # if frame_count % 50:\n",
    "                #     # check if run time > time_limit\n",
    "                #     if time.time() - start_time > time_limit:\n",
    "                #         break\n",
    "                    \n",
    "            pbar.close()\n",
    "        # prof.print_stats()\n",
    "\n",
    "        vs.release()\n",
    "        out.release()\n",
    "        print(f'finished part={part}, {frame_count} frames ({frame_count/total_frames*100}%)')\n",
    "    \n",
    "    def get_id(self):\n",
    "        return id(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:08:06,082\tWARNING worker.py:1986 -- Warning: The actor WorkerActor is very large (64 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(WorkerActor, 2ece7db0e5e6e1af6edb3f2401000000)-->./input/videos/low_res_2_parts/low_res_2_part0.mp4 to ./output/videos/low_res_2_multiproc/low_res_2_BSRGAN_part0.mp4\n",
      "Actor(WorkerActor, 393664afe86cc6ec013e4be601000000)-->./input/videos/low_res_2_parts/low_res_2_part1.mp4 to ./output/videos/low_res_2_multiproc/low_res_2_BSRGAN_part1.mp4\n",
      "Actor(WorkerActor, 9743942dacfd13cdc82c5e1401000000)-->./input/videos/low_res_2_parts/low_res_2_part2.mp4 to ./output/videos/low_res_2_multiproc/low_res_2_BSRGAN_part2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m 2023-06-03 12:08:09,570\tERROR serialization.py:387 -- No module named 'models'\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m   File \"/home/asutosh/miniconda3/envs/env_pytorch/lib/python3.8/site-packages/ray/_private/serialization.py\", line 385, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m   File \"/home/asutosh/miniconda3/envs/env_pytorch/lib/python3.8/site-packages/ray/_private/serialization.py\", line 268, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m   File \"/home/asutosh/miniconda3/envs/env_pytorch/lib/python3.8/site-packages/ray/_private/serialization.py\", line 223, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m   File \"/home/asutosh/miniconda3/envs/env_pytorch/lib/python3.8/site-packages/ray/_private/serialization.py\", line 213, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m ModuleNotFoundError: No module named 'models'\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m 2023-06-03 12:08:09,572\tERROR worker.py:844 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::WorkerActor.__init__()\u001b[39m (pid=10647, ip=192.168.29.117, repr=<__main__.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x7f9034905070>)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m   At least one of the input arguments for this task could not be computed:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m ray.exceptions.RaySystemError: System error: No module named 'models'\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=10647)\u001b[0m ModuleNotFoundError: No module named 'models'\n"
     ]
    }
   ],
   "source": [
    "workers = []\n",
    "for i in range(threads):\n",
    "    output_path = get_output_file_path(file_name, model_name, f'part{i}', output_root_path, ext)\n",
    "    input_path = f'{input_video_parts_root}/{file_name}_part{i}.{ext}'\n",
    "    workers.append(WorkerActor.remote(modelObjRef, input_path, output_path))\n",
    "    print(f'{workers[-1]}-->{input_path} to {output_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upscale video multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in workers:\n",
    "    worker.upscale.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def concatenate_videos(new_video_path, codec, fps, resolution, videos):\n",
    "    video = cv2.VideoWriter(new_video_path, codec, fps, resolution)\n",
    "\n",
    "    for v in tqdm(videos):\n",
    "        curr_v = cv2.VideoCapture(v)\n",
    "        while curr_v.isOpened():\n",
    "            r, frame = curr_v.read()\n",
    "            if not r:\n",
    "                break\n",
    "            video.write(frame)\n",
    "\n",
    "    video.release()\n",
    "    print(f'video combined at {new_video_path}')\n",
    "\n",
    "files_sorted = list(map(str, sorted(Path(output_root_path).iterdir(), key=os.path.basename)))\n",
    "concat_path = get_output_file_path(file_name, model_name, 'all', output_root_path, ext)\n",
    "concatenate_videos(concat_path, codec, frame_rate, out_resolution, files_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import AudioFileClip\n",
    "\n",
    "video = moviepy.editor.VideoFileClip(input_video)\n",
    "audio = video.audio\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_out = moviepy.editor.VideoFileClip(concat_path)\n",
    "audio = audio.volumex(2)\n",
    "video_with_audio = video_out.set_audio(audio)\n",
    "output_video_audio = get_output_file_path(file_name, model_name, 'final', output_root_path, ext)\n",
    "video_with_audio.write_videofile(output_video_audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
